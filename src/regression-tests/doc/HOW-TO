HOW-TO

This document describes how to use this regression test system.

The regression tests are organized around goals. Each goal has its own
directory under regression-tests/goals, and each goal directory
contains (or should contain) a README file explaining the goal in
question.

Two goal directories are seq-hof and par-ptup. Their README files
(subject to improvement, of course) are currently

  Tests of handwritten versions of common functional programming
  combinators like map and filter.

and

  Some tests of Manticore programs that use parallel tuples.

respectively.

ADDING NEW GOALS AND TESTS

To add a new test to the batch, first peruse the existing goal
directories and their READMEs to see if your test fits into an
existing category. If not, create a new directory under goals and a
corresponding README.

To create a new individual test, you must create two files: one called
program.pml, and one called program.ok. The latter contains the expected
results of the former.

For the sake of argument, assume we'd like to test multiplication on
parallel tuples. First we note this belongs in the existing directory
goals/par-ptup. We save the following two files into par-ptup:

  mult.pml

    fun mult (m:int, n:int) = m*n 
    val prod = mult (| 2, 3 |)
    val _ = Print.printLn (itos prod)

  mult.ok

    6

Then you can simply use the run-tests script (instructions
follow). The program will pick up all the programs inside directories
inside regression-tests/goals, so no further action needs to be taken.

Programs without observable effects -- in other words, those whose ok
files are empty -- are not recommended for tests, although they will
flow through the testing process without trouble.

RUNNING THE TESTS

There is a script in regression-tests/sml-scripts/bin called
run-tests. Executing it will populate regression-tests/reports with
new test results. Run this command with the name of a local file where
you'd _also_ like the test result file to go. For example, I (Adam)
run this command with

  ./run-tests /home/adamshaw/MCResults/current/results.html

Working hand-in-hand with NFS, I am then able to view the results on
my local machine, which can't directly run the Manticore compiler
(and conversely, the machine running the Manticore compiler can't run
a web browser for me).

Note you'll want the stylesheet results.css to be one directory up
from the generated file so that it looks nice. (It will be legible
even without it.) You can grab a copy of results.css from
regression-tests/reports/.

The HTML report generated by the test run includes, for each test
program, the current result and as many as four most recent test
results, if they exist. This way one can get a sense of whether the
compiler is at present better or worse than previous incarnations.

FRAGILITY

If the name (i.e., filename) of a test is changed, then its ties to
previous test runs will be severed. More concretely, if there is a
test called map.pml whose filename is changed to pair-map.pml, then
the system will not be able to relate its current run to previous
runs. One could imagine building a more robust infrastructure that
addressed this weakness, but I chose not to invest the time.

Along similar lines, if the content of a test is changed, the testing
facilities will not indicate or "know" that a test is not testing
exactly what it used to.

Therefore, it should be considered best practice to add a test once
and only once to the batch, and leave its name and contents alone;
that is, add new, freshly-named tests in favor of modifying old ones.

KNOWN ISSUES

- Where should the archive files go? In svn? In some common spot on
NFS?

- It might be worth considering whether backends other than HTML would
be desirable.

- Paths and filenames are not yet all abstracted away from particular
syntactic conventions. i.e., I should be using OS.Path.joinBaseExt
instead of (b ^ "." ^ e).
